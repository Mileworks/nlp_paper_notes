more refer:

注意力机制概述, Aston Zhang [site](<https://discuss.gluon.ai/t/topic/13075>)

Attention in Deep learning, ICML2019, Alex Smola and Aston Zhang [site](https://mp.weixin.qq.com/s/ZSzHOu6uowRSoWrqB7vOaQ) 



1,为啥每个词语都需要一个aspect embedding ?
学到抽象的service or food ?
2,只能做简单的事情 - 模型
3,skip connection
避免信息丢失
4,attention
强化信息
5,CLS...学到抽象的...
6,注意力池化
7,BERT 双向变换器编码表征
调整CLS获得句子表示的注意力
...
8, 
少参
结构化


四元数变换器 - tay et al. 19

9, Transformer-XL 
   XLNet


10, truncated BPTT - rnn tensorflow是采用哪种?

交叉验证
快速调优 - how ？？？



### Tools

+ <https://github.com/philipperemy/keras-attention-mechanism>
+ <https://github.com/openai/sparse_attention>



### Survey

An Attentive Survey of Attention Models, Sneha Chaudhari, Gungor Polatkan, Rohan Ramanath, Varun Mithal,  IJCAI 2019 [arxiv](<https://arxiv.org/abs/1904.02874>) 

### Paper

Attention Is All You Need, Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin, [arxiv](<https://arxiv.org/abs/1706.03762>) :star::star::star::star:

Attention is not Explanation, Sarthak Jain, Byron C. Wallace, NAACL 2019, [arxiv](<https://arxiv.org/abs/1902.10186>) 

Is Attention Interpretable?, Sofia Serrano, Noah A. Smith, ACL 2019 [arxiv](<https://arxiv.org/abs/1906.03731>) 



