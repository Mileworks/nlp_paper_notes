[100 Must-Read NLP Papers](https://github.com/mhagiwara/100-nlp-papers)



## Segmentation, Tagging, Parsing, SRL

* Michael Collins: [Head-Driven Statistical Models for Natural Language Parsing](http://www.cs.columbia.edu/~mcollins/papers/thesis.ps), PhD Dissertation, University of Pennsylvania, 1999.
* Michael Collins: [Discriminative Training Methods for Hidden Markov Models: Theory and Experiments with Perceptron Algorithms](http://www.cs.columbia.edu/~mcollins/papers/tagperc.pdf), EMNLP 2002. *(Received Best Paper Award)* 



* Simple and Accurate Dependency Parsing Using Bidirectional LSTM Feature Representations, Eliyahu Kiperwasser, Yoav Goldberg, TACL 2016 [arxiv](https://arxiv.org/abs/1603.04351) 
* Deep Semantic Role Labeling: What Works and Whatâ€™s Next, He Luheng, Lee Kenton, Lewis Mike, [Zettlemoyer Luke](https://scholar.google.com/citations?user=UjpbO6IAAAAJ&hl=zh-CN&oi=sra), ACL 2017 [paper](https://aclanthology.info/papers/P17-1044/p17-1044) 



## Machine Translation & Transliteration, Sequence-to-Sequence Models

+ Ashish Vaswani, et al.: Attention Is All You Need, 2017. 


## Neural Models

- Matthew E. Peters, et al.: Deep contextualized word representations, 2018. [arxiv](https://arxiv.org/abs/1802.05365?context=cs) 
- Jacob Devlin, et al.: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding, 2018. [arxiv](https://arxiv.org/abs/1810.04805?context=cs) 
- 