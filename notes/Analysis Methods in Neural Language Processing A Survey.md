### key points

+ review analysis methods in neural language processing, categorize them according to
  prominent research trends, highlight existing limitations, and point to potential directions
  for future work.
+ As end to end systems are gaining prevalence, one may point to two trends. First, some push back against the abandonment of linguistic knowledge and call for incorporating it inside the networks in different ways. Others strive to better understand how neural language processing models work (interpretability). Much of the analysis work thus aims to understand how linguistic concepts that were common as features in NLP systems are captured in neural networks.
+ Elman (1989) introduced analysis methods: from visualizing network activations in time, through clustering words by hidden state activations, to projecting representations to dimensions that emerge as capturing properties like sentence number or verb valency. But his work was limited in some ways, such as evaluating generalization or various linguistic phenomena.

#### What linguistic information is captured in neural networks

##### which methods are used for conducting the analysis

+ The most common approach - predict linguistic properties from activations of the neural network.
+ Other methods for finding correspondences between parts of the neural network and certain
  properties include counting how often attention weights agree with a linguistic property or directly/indirectly computing correlations between neural network activations and some property,

##### what kind of linguistic information is sought

Different kinds of linguistic information have been analyzed (ranging from basic properties like sentence length, word position, word presence, or simple word order, to morphological, syntactic,
and semantic information), (while it is difficult to synthesize a holistic picture from this diverse body of work) it appears that neural networks are able to learn a substantial amount of information on various linguistic phenomena.

But:

+ These models are especially successful at capturing frequent properties, while some
  rare properties are more difficult to learn.
+ the hierarchical nature of the learned representations.
+ models trained with latent trees perform better on natural language inference (NLI) than ones trained with linguistically-annotated trees. Moreover, the trees in these models do not resemble syntactic trees corresponding to known linguistic theories, which casts doubts on the importance of syntax learning in the underlying neural network.

##### which objects in the neural network are being investigated.

Various neural neural network components were investigated, including word embeddings, RNN hidden states or gate activations, sentence embeddings, and attention weights in sequence-to-sequence (seq2seq) models.

##### limitations

From a methodological point of view, most of the relevant analysis work is concerned with correlation: how correlated are neural network components with linguistic properties? What may be lacking is a measure of causation (causal inference): how does the encoding of linguistic properties affect the system output.

+ “embedding models are able to encode divergent linguistic information but have limits on how this information is surfaced.”

  The classification approach may find that a certain amount of linguistic information is captured in the neural network. However, this does not necessarily mean that the information is used by the network.

#### the generation and use of adversarial examples to probe weaknesses of neural networks.

In the text domain, it's difficult to generate adversary examples for two reasons: it's not clear how to measure the distance between the original and adversarial examples, and minimizing this distance cannot be easily formulated as an optimization problem.

There are several methods for handling these difficulties:<br>1. the adversary’s knowledge<br>2. the specificity of the attack<br>3. the linguistic unit being modified<br>4. the task on which the attacked model was trained.

##### the adversary’s knowledge

Adversarial examples can be generated using access to model parameters, also known as white-box attacks, or without such access, with black-box attacks.

white-box attacks:<br>1. compute gradients with respect to the input word embeddings, and perturb the embeddings, and search for the closest word embedding in a given dictionary.<br>2. computed gradients with respect to input word embeddings to identify and rank words to be modified.<br>3. an alternative method by representing text edit operations in vector space (e.g., a binary
vector specifying which characters in a word would be changed) and approximating the change
in loss with the derivative along this vector.

black-box attacks:<br>1. text edits that are thought to be natural or commonly generated by humans, such as typos, misspellings, and so on. They need the model prediction score to identifying the important tokens, then modify characters with common edit operations.<br>2. population-based genetic algorithm for crafting adversarial examples for text classification,
by maintaining a population of modifications of the original sentence and evaluating fitness of
modifications at each generation. They do not require access to model parameters, but do use prediction scores.<br>3. generative adversarial networks (GANs) to minimize the distance between latent representations
of input and adversarial examples, and performed perturbations in latent space.

##### the specificity of the attack

Adversarial attacks can be classified to targeted vs. non-targeted attacks. A targeted attack specifies a specific false class, while a non-targeted attack only cares that the predicted
class is wrong.

Methods for generating targeted attacks in NLP could possibly take more inspiration from adversarial attacks in other fields. For instance, in attacking malware detection systems, several studies developed targeted attacks in a black-box scenario. 

Targeted attacks:<br>1. specified a desired class to fool a text classifier, specified words or captions to generate in an image captioning model.<br>2. specific words to omit, replace, or include when attacking seq2seq models.<br>3. A black-box targeted attack for MT was proposed by Zhao et al. (2018c), who used GANs to search for attacks on Google’s MT system after mapping sentences into continuous space with adversarially regularized autoencoders.

Non-targeted attacks:<br>1. ...<br>2. ...

##### the linguistic unit being modified

1.modifications at the character- and/or word-level<br>2.adding sentences or text chunks<br>3.generating paraphrases with desired syntactic structures

##### the task on which the attacked model was trained.

high-level language understanding tasks, such as text classification (including sentiment analysis) and reading comprehension, while work on text generation focuses mainly on MT.

low-level language processing tasks, although one can mention morphological tagging and spelling correction.

##### Coherence & perturbation measurement

In the text domain, measuring distance is not as straightforward and even small changes to the text may be perceptible by humans. Thus, evaluation of attacks is fairly tricky.

Some studies imposed constraints on adversarial examples to have a small number of edit operations. Others ensured syntactic or semantic coherence in different ways, such as filtering replacements by word similarity or sentence similarity, or by using synonyms and other word lists.

Given the inherent difficulty in generating imperceptible changes in text, more such evaluations are needed.



