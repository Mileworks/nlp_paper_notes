

### Method

#### 监督学习

character-based

基本思想：将分词过程看作是字的分类问题。该方法认为，每个字在构造一个特定的词语时都占据着一个确定的构词位置（即词位）。假定每个字只有4个词位：词首（B）、词中（M）、词尾（E）和单独成词（S），那么，每个字归属一特定的词位。



Qi Zhang, Xiaoyu Liu, Jinlan Fu. Neural Networks Incorporating Dictionaries for Chinese Word Segmentation. AAAI 2018

<https://github.com/fudannlp16/CWS_Dict>



#### 无监督/半监督学习



[漫话中文自动分词和语义识别（上）：中文分词算法](http://www.matrix67.com/blog/archives/4212)

[漫话中文自动分词和语义识别（下）：句法结构和语义结构](http://www.matrix67.com/blog/archives/4870)

Chinese word segmentation algorithm without corpus（无需语料库的中文分词）

<https://github.com/Moonshile/ChineseWordSegmentation>



中文三段式机械分词算法

<https://github.com/GeorgeBourne/grid>

### Tools

+ tire tree

  <http://www.hankcs.com/program/java/tire-tree-participle.html>

+ jieba

  [jieba分词学习笔记（一）](https://segmentfault.com/a/1190000004061791)

  [jieba分词学习笔记（二）](https://segmentfault.com/a/1190000004065927)

  [jieba分词学习笔记（三）](https://segmentfault.com/a/1190000004085949)

  <https://blog.csdn.net/hhtnan/article/details/77650128>

  <https://blog.csdn.net/hhtnan/article/details/76586693>

  [对Python中文分词模块结巴分词算法过程的理解和分析](https://blog.csdn.net/rav009/article/details/12196623) 

<div align="center">
<img src="https://github.com/bifeng/nlp_paper_notes/raw/master/image/jieba.png" width="600" height="400" alt="jieba flow chart"></img>
</div>
+ zhseg

  <http://www.zhseg.com/>

  <https://github.com/chinese-segmentation/zhseg>

  

  

  