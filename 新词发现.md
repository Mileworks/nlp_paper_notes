







refer:

[反作弊基于左右信息熵和互信息的新词挖掘](https://zhuanlan.zhihu.com/p/25499358)

[互联网时代的社会语言学：基于SNS的文本数据挖掘](http://www.matrix67.com/blog/archives/5044)





### Question

+ 如何借助已有词典进行新词发现？

  首先，利用细粒度的分词器初步分词

  其次，基于分词结果构建新词

  

+ 如何处理人名、公司名等实体？

  

+ 如何处理频率较低的新词？

  

+ 如何处理单个字成词？

  

+ `假设二：词语是可以灵活运用的，有丰富的搭配` 仅考虑左右边一个字情形？

+ 

  



### Competition

+ [“AIIA”杯-国家电网-电力专业领域词汇挖掘](<https://www.datafountain.cn/competitions/320>)

  [第2组-张忠辉]()

  > 1. 种子词
>
  >    （频次、左右熵、互信息）
  >
  >    从完整性，独立性，紧密度，构词模式等方面计算成词概率
  >
  >    完整性 - 词语内部的各个成分不能构成语法分析结构中的一棵完整子树的词语降权
  >
  >    独立性 - 左右熵较高，表明上下文多 样性较高，成词概率越大。
  >
  >    紧密度 - 词语内部成分互信息高，而与相邻 外部成分互信息低，则成词概率较 大。 计算词语成分位于开头、结尾的概 率，作为成词概率计算的特征。 少数词语的语法属性句子级别的， 如果与句子内部的成分结合，则成 词概率较低。
  >
  >    构词模式 - 对于“湖南电力公司”这样 的词语，因为可以被构词模 式“【地区】【机构后缀】 ”很好地概括，我们降低其作为领域词汇的概率
  >
  > 2. 训练
  >
  >    正样本 - 领域词汇
  >
  >    负样本 - 背景词汇
  >
  > 3. 排序
  >
  >    词频、词语长度、关键词、预测概率三个因素
  >
  >    词频 - 词语在领域语料中词频越高， 在背景语料中词频越低，则得 分越高
  >
  >    词语长度 - 越长的词语，往往包含信息越 丰富。为了补偿频次方面的劣 势，需要对其重要度进行提权 。
  >
  >    关键词 - 对于论文语料，有专门的“关键 词”栏目可以用于识别关键词， 对于该栏目中的词汇要提权。
  >
  >    预测概率 - 前述的领域判定模型的得分将 影响最终的排序。

  [5th](<https://github.com/yuquanle/CCF-AIIA-VocabularyMining>) 

  https://github.com/zhpmatrix/nlp-competitions-list-review/blob/master/DataFountain_AIIA杯_国家电网_电力专业领域词汇挖掘.md

  https://spaces.ac.cn/archives/6540





### Tools

+ <https://github.com/Ushiao/wordiscovery>

  [site](https://www.flykun.com/2017-11-27/词频、互信息、信息熵发现中文新词.html)

+ <https://github.com/wzc118/topwords>

  



### Paper

+ Deng K, Bol P K, Li K J, et al. On the unsupervised analysis of domain-specific Chinese texts[J]. Proceedings of the National Academy of Sciences, 2016. [paper](<https://dash.harvard.edu/bitstream/handle/1/27303651/PNAS-2016-Deng-1516510113.pdf?sequence=1&isAllowed=y>) | [site](<https://baijiahao.baidu.com/s?id=1622711692639477068&wfr=spider&for=pc>) 



### Blog

+ 新词发现系列

  [新词发现的信息熵方法与实现](https://spaces.ac.cn/archives/3491)

  [【中文分词系列】 1. 基于AC自动机的快速分词](https://spaces.ac.cn/archives/3908)

  [【中文分词系列】 2. 基于切分的新词发现](https://spaces.ac.cn/archives/3913)

  [【中文分词系列】 3. 字标注法与HMM模型](https://spaces.ac.cn/archives/3922)

  [【中文分词系列】 4. 基于双向LSTM的seq2seq字标注](https://spaces.ac.cn/archives/3924)

  [【中文分词系列】 5. 基于语言模型的无监督分词](https://spaces.ac.cn/archives/3956)

  [【中文分词系列】 6. 基于全卷积网络的中文分词](https://spaces.ac.cn/archives/4195)

  [【中文分词系列】 7. 深度学习分词？只需一个词典！](https://spaces.ac.cn/archives/4245)

  [【中文分词系列】 8. 更好的新词发现算法](https://spaces.ac.cn/archives/4256)

  [重新写了之前的新词发现算法：更快更好的新词发现](https://spaces.ac.cn/archives/6920)

  > 从相关性角度解读分词
  >
  > 三步：
  >
  > step 1 统计 - enumerate
  >
  > 构建ngrams，筛选高凝固度的集合作为分词词典(G1)
  >
  > step 2 切分 -
  >
  > 统计分词结果，筛选出高频词语集合(G2)
  >
  > （比如两字的“共和”不会出现在高凝固度集合中，所以会切开（比如“我一共和三个人去玩”，“共和”就切开了），但三字“共和国”出现在高凝固度集合中，所以“中华人民共和国”的“共和”不会切开；）
  >
  > step 3 回溯 - filter
  >
  > 过滤高频词语集合，即为最终的新词集合(G3)
  >
  > 示例：因为“各项”和“项目”都出现高凝固度的片段中，所以第二步我们也不会把“各项目”切开，但我们不希望“各项目”成词，因为“各”跟“项目”的凝固度不高（“各”跟“项”的凝固度高，不代表“各”跟“项目”的凝固度高），所以通过回溯，把“各项目”移除（只需要看一下“各项目”在不在原来统计的高凝固度集合中即可，所以这步计算量是很小的）

+ [非主流自然语言处理——遗忘算法系列（二）：大规模语料词库生成](<http://www.52nlp.cn/forgetnlp2>)

+ 

### Summary

#### assumptions/rules

notes: 概率的前提条件，词语数量具有统计意义。所以，需要足够**语料**以保证方法的有效性。

假设一：词语是不可分割的单元，经常一起出现

（成词标准一：文本片段的内部凝固程度 - 互信息 (mutual information) & 点间互信息 (pointwise mutual information)）

互信息 (mutual information) 
$$
MI(X,Y) = \sum_{x\in X,y\in Y} p(x,y) \log \frac{p(x,y)}{p(x)p(y)}
$$
点间互信息 (pointwise mutual information)
$$
PMI(x,y)=\log \frac{p(x,y)}{p(x)p(y)}
$$
当 x, y 相互独立时，则 p(x , y) = p(x)p(y), PMI 则为0。否则，当x, y越相关，则 p(x , y) 相比于 p(x)p(y) 越大, 那 PMI 也就越大。

p(x, y)越大表示两个片段一起出现的概率越大，即x, y的内部凝聚力。



示例： “电影院”与“的电影”

在人人网用户状态中，“的电影”出现了 389 次，“电影院”只出现了 175 次，然而我们却更倾向于把“电影院”当作一个词，因为直觉上看，“电影”和“院”凝固得更紧一些。

在整个 2400 万字的数据中，“电影”一共出现了 2774 次，出现的概率约为 0.000113 。“院”字则出现了 4797 次，出现的概率约为 0.0001969 。如果两者之间真的毫无关系，它们恰好拼在了一起的概率就应该是 0.000113 × 0.0001969 ，约为 2.223 × 10-8 次方。但事实上，“电影院”在语料中一共出现了 175 次，出现概率约为 7.183 × 10-6 次方，是预测值的 300 多倍。类似地，统计可得“的”字的出现概率约为 0.0166 ，因而“的”和“电影”随机组合到了一起的理论概率值为 0.0166 × 0.000113 ，约为 1.875 × 10-6 ，这与“的电影”出现的真实概率很接近——真实概率约为 1.6 × 10-5 次方，是预测值的 8.5 倍。计算结果表明，“电影院”更可能是一个有意义的搭配，而“的电影”则更像是“的”和“电影”这两个成分偶然拼到一起的。

为了算出一个文本片段的凝合程度，我们需要<u>枚举</u>它的凝合方式——这个文本片段是由哪两部分组合而来的。令 p(x) 为文本片段 x 在整个语料中出现的概率，那么我们定义“电影院”的凝合程度就是 p(电影院) 与 p(电) · p(影院) 比值和 p(电影院) 与 p(电影) · p(院) 的比值中的较小值，“的电影”的凝合程度则是 p(的电影) 分别除以 p(的) · p(电影) 和 p(的电) · p(影) 所得的商的<u>较小值</u>。(“电影”比“影院”出现的概率大，所以应取商之较小值)



假设二：词语是固定搭配，因此词语内部之间的搭配较少（:question:）

即文本片段的内部自由程度较低



假设三：词语是可以灵活运用的，有丰富的搭配

（成词标准三： 文本片段的外部自由程度 - 熵 (Entropy)）

熵 (Entropy)
$$
H(x) = - \sum_{x \in X} p(x) \log p(x)
$$
通过计算一个候选词左边和右边的信息熵来反映了一个词是否有丰富的左右搭配（所处语境的丰富程度），如果达到一定阈值则我们认为两个片段可以成为一个新词。



示例：“被子”和“辈子”

在人人网用户状态中，“被子”一词一共出现了 956 次，“辈子”一词一共出现了 2330 次，两者的右邻字集合的信息熵分别为 3.87404 和 4.11644 ，数值上非常接近。但“被子”的左邻字用例非常丰富：用得最多的是“晒被子”，它一共出现了 162 次；其次是“的被子”，出现了 85 次；接下来分别是“条被子”、“在被子”、“床被子”，分别出现了 69 次、 64 次和 52 次；当然，还有“叠被子”、“盖被子”、“加被子”、“新被子”、“掀被子”、“收被子”、“薄被子”、“踢被子”、“抢被子”等 100 多种不同的用法构成的长尾⋯⋯所有左邻字的信息熵为 3.67453 。但“辈子”的左邻字就很可怜了， 2330 个“辈子”中有 1276 个是“一辈子”，有 596 个“这辈子”，有 235 个“下辈子”，有 149 个“上辈子”，有 32 个“半辈子”，有 10 个“八辈子”，有 7 个“几辈子”，有 6 个“哪辈子”，以及“n 辈子”、“两辈子”等 13 种更罕见的用法。所有左邻字的信息熵仅为 1.25963 。因而，“辈子”能否成词，明显就有争议了。

为了算出一个文本片段的自由运用程度，不妨定义为它的左邻字信息熵和右邻字信息熵中的<u>较小值</u>。



在实际运用中你会发现，文本片段的凝固程度和自由程度，两种判断标准缺一不可。只看凝固程度的话，程序会找出“巧克”、“俄罗”、“颜六色”、“柴可夫”等实际上是“半个词”的片段；只看自由程度的话，程序则会把“吃了一顿”、“看了一遍”、“睡了一晚”、“去了一趟”中的“了一”提取出来，因为它的左右邻字都太丰富了。



假设四：

左邻或右邻是特殊字符（如“”《》等），提高成词概率

含停用词，降低成词概率



#### method

step 1. 种子词

如何生成种子词？

如何保证种子词的分布平衡？

LHS-like sampling



step 2. 候选词排序

如何避免（由于种子词引发的）语义漂移现象？

如何确定阈值？





1. 点互信息 + 左右信息熵之较小者 
2. 点互信息 - 左右片段信息熵之较小者 + 左右信息熵之较小者

3. TF  *（...）

4. TF-IDF  * （...）

左右信息熵之较小者 or 左右信息熵之乘积







##### unsupervised

implement method 1:

 我们把文本中出现过的所有长度不超过 d 的子串都当作潜在的词（即候选词，其中 d 为自己设定的候选词长度上限，我设定的值为 5 ），再为出现频数、凝固程度和自由程度各设定一个阈值，然后只需要提取出所有满足阈值要求的候选词即可。为了提高效率，我们可以把语料全文视作一整个字符串，并对该字符串的所有后缀按字典序排序。下表就是对“四是四十是十十四是十四四十是四十”的所有后缀进行排序后的结果。实际上我们只需要在内存中存储这些后缀的前 d + 1 个字，或者更好地，只储存它们在语料中的起始位置。

> 十
> 十十四是十四四十是四十
> 十是十十四是十四四十是四十
> 十是四十
> 十四是十四四十是四十
> 十四四十是四十
> 是十十四是十四四十是四十
> 是十四四十是四十
> 是四十
> 是四十是十十四是十四四十是四十
> 四十
> 四十是十十四是十四四十是四十
> 四十是四十
> 四是十四四十是四十
> 四是四十是十十四是十四四十是四十
> 四四十是四十

​    这样的话，相同的候选词便都集中在了一起，从头到尾扫描一遍便能算出各个候选词的频数和右邻字信息熵。将整个语料逆序后重新排列所有的后缀，再扫描一遍后便能统计出每个候选词的左邻字信息熵。另外，有了频数信息后，凝固程度也都很好计算了。这样，我们便得到了一个无需任何知识库的抽词算法，输入一段充分长的文本，这个算法能以大致 O(n · logn) 的效率提取出可能的词来。



implement method 2:

未登录词分析步骤：

1. 首先需要对输入文本进行清洗和分词，如果没有任何分词词库的情况下，可以直接将文本按照字符分割成一个**单字符**集合。

（**单字符**，可以减少分词错误引入的误差～）

2. 将字符两两组合作为**候选词**，因为需要前缀和后缀来计算信息熵，因此我们需要存储长度为 3 的片段。由于后续涉及到前后缀的查找和词频的统计，最终我们选定了[Trie 树](https://link.zhihu.com/?target=https%3A//github.com/julycoding/The-Art-Of-Programming-By-July/blob/master/ebook/zh/06.09.md)来存储数据。用 3-gram 序列构建前缀 Trie 树和后缀 Trie 树，Trie 树以单个字符为节点，每个节点记录从根节点到当前节点构成词汇出现的频次。

（把文本中出现过的所有长度不超过 d 的子串都当作**候选词**，其中 d 为候选词长度上限，通常为3或5）

3. 查询 Trie 树，获取前缀和后缀的频次列表，计算**候选词的左右信息熵**以及**候选词构成片段的信息熵**。因为涉及到的信息熵比较多，我们对每个信息熵作如下区分标记（Candidate 为候选词，left 为左边构成的片段，right 为右边构成的片段）：omit

1. 查询 Trie 树，获取候选词的词频以及左右片段的词频。有了词频之后便可以很方便的得到实际出现概率 p(x,y) 和期望出现概率 p(x)p(y)，从而计算出凝合度和互信息。为了防止冷启动前出现过高的概率 p，我们预先跑了一些基础词汇的词频，在词频库的基础上，可以保证刚开始的时候就能拿到比较正确的概率 p。
2. 为了增加准确率，可以设定**词频**、**凝固程度、自由程度**的阈值，排除一些低词频和低凝合度的词汇。所有满足条件的词汇，通过调用 jieba 的 suggest_freq 函数来使其能被分出来。

（**词频**、**凝固程度**和**自由程度**各设定一个阈值，词频大于 2, PMI 大于等于 24...）

3. 针对新词成词的特点, 在实际应用中，我们为每一个备选单词计算了一个分数，表示在当前上下文成为新词的可能性。计算公式如下：
   $$
   score = PMI - min(h\_r\_l, h\_l\_r) + min(h\_l,h\_r)
   $$
   该分数由三个部分组成：

   1）点间互信息：点间互信息越高，内部聚合程度越高

   2）两个单词片段信息熵 h_r_l 和 h_l_r 的最小值：这个数值越大，则意味着两个单词一起出现的可能性越小

   3）单词左右信息熵的最小值：这个数值越大就表示着候选词出现的语境越多，越有可能成词

   因此，分数越高表示成词的可能性越大。

最后根据词频和分数的乘积排序，筛选出 top-n的词汇作为新词。



说明：以“淘宝客”为例

h_l - 即“淘宝客”的左邻信息熵

h_r - 即“淘宝客”的右邻信息熵

h_r_l - 即词语右边构成片段的左邻信息熵，“宝客”、“客”的左邻信息熵

h_l_r - 即词语左边构成片段的右邻信息熵，“淘宝”、“淘”的右邻信息熵



注意：

若单词左右信息熵 ( h_l, h_r ) 为 0 ，迭代一轮，确认是否可能与左右的片段组成新词。 比如 “淘宝客” 这个词，先被分成了 “淘”、“宝”、“客”，在检测 “淘宝” 的时候，会发现它的右信息熵为 0，因此 “淘宝” 在当前上下文可能是另一个词的片段，所以通过下一轮迭代，检测 “淘宝” 和 “客” 能否成词。



##### supervised













